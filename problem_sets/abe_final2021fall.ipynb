{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"abe_final2021fall.ipynb","provenance":[{"file_id":"1IAnpQCRt7KwqeqGirSwbNBzFSkSF4wKR","timestamp":1639332232978}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-JYG5qnnWM9f"},"source":["# Final Exam"]},{"cell_type":"markdown","metadata":{"id":"-YiFzYF2WM9g"},"source":["Exam is open book, open note, and open Google. You are not allowed outside\n","help from another person, however. All work, including coding, must be yours alone. Remember to turn in both the written portion and this coding portion. Turn in this coding portion by downloading your completed Colab notebook as a .ipynb file and submitting it via Learning suite. To get full credit, the completed notebook should be able to run top to bottom, producing the results asked for in the prompt below."]},{"cell_type":"markdown","metadata":{"id":"EbV7CV3Pz2st"},"source":["## The Question"]},{"cell_type":"markdown","metadata":{"id":"MkCjCsX9WM9g"},"source":["An important question in public economics is whether eligibility for publicly-provided health insurance (Medicaid) increases individuals' use of the healthcare system. In this final exam you will use machine learning to estimate the causal effect of Medicaid eligibility on how many times an individual visits the doctor. In the Econ 484 Google Drive \"datasets\" folder you will find a dataset called \"oregon.csv\" and the associated codebook \"oregon_codebook.txt\" that gives some information about each of the variables. The outcome variable is the number of primary care doctor visits. The treatment variable is an indicator variable for ever enrolled in Medicare. A possible instrumental variable is an indicator for being selected in the Oregon Medicaid Experiment lottery. Additional covariates are number of household members, gender, age, employment status, and race indicators."]},{"cell_type":"markdown","source":["# Data Manipulation"],"metadata":{"id":"f6c3U2dq99SS"}},{"cell_type":"code","source":["%matplotlib inline\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd '/content/gdrive/My Drive/Econ 484/datasets'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sfy8Y5vr0cNm","executionInfo":{"status":"ok","timestamp":1639518899218,"user_tz":420,"elapsed":1327,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"73898ee0-9f7a-4b26-b2b6-63ecf744e9cc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Econ 484/datasets\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.read_csv('oregon.csv') #read in the data\n","df = df.dropna(axis=0) #drop rows with missing values\n","print(df.columns) #print columns and first few rows to get a sense of the data\n","df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"V46siruD02De","executionInfo":{"status":"ok","timestamp":1639518899487,"user_tz":420,"elapsed":273,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"f15541cc-0970-434d-b373-22fa11e698ff"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['wonlottery', 'numhhmembers', 'female_6m', 'birthyear_6m',\n","       'employ_hrs_6m', 'race_hisp_6m', 'race_white_6m', 'race_black_6m',\n","       'race_amerindian_6m', 'race_asian_6m', 'race_pacific_6m',\n","       'race_other_qn_6m', 'doctor_visits', 'hasmedicaid'],\n","      dtype='object')\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>wonlottery</th>\n","      <th>numhhmembers</th>\n","      <th>female_6m</th>\n","      <th>birthyear_6m</th>\n","      <th>employ_hrs_6m</th>\n","      <th>race_hisp_6m</th>\n","      <th>race_white_6m</th>\n","      <th>race_black_6m</th>\n","      <th>race_amerindian_6m</th>\n","      <th>race_asian_6m</th>\n","      <th>race_pacific_6m</th>\n","      <th>race_other_qn_6m</th>\n","      <th>doctor_visits</th>\n","      <th>hasmedicaid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>1968.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>1962.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1954.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1946.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1982.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    wonlottery  numhhmembers  ...  doctor_visits  hasmedicaid\n","7            0             2  ...            0.0            0\n","26           0             2  ...            0.0            0\n","27           0             1  ...            1.0            0\n","41           1             1  ...            0.0            0\n","77           1             1  ...            2.0            0\n","\n","[5 rows x 14 columns]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#get outcome, treatment, instrument, and covariates subset\n","\n","y= df['doctor_visits']\n","D= df['hasmedicaid']\n","Z= df['wonlottery']\n","X= df.loc[:,[x for x in df.columns if x not in ('doctor_visits','hasmedicaid','wonlotter')]]\n","\n","print(D.shape)\n","print(y.shape)\n","print(X.shape)\n","print(Z.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xT7_Ct1r2Kly","executionInfo":{"status":"ok","timestamp":1639518899503,"user_tz":420,"elapsed":28,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"0306f420-a54f-4de7-9888-8b835c1289b9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(4666,)\n","(4666,)\n","(4666, 12)\n","(4666,)\n"]}]},{"cell_type":"code","source":["#standardize X matrix. Leave y, D and Z as is since D and Z are dummies and y is an outcome we want to leave as is\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler() #create scaler object\n","scaler.fit(X) #feed the scaler object the x\n","x_scaled = scaler.transform(X)"],"metadata":{"id":"Ka9PRM4U4Iq2","executionInfo":{"status":"ok","timestamp":1639518899956,"user_tz":420,"elapsed":478,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlRWfRg-0HUe"},"source":["## The Task"]},{"cell_type":"markdown","metadata":{"id":"-WfUoTft0L2i"},"source":["Estimate the causal effect of Medicaid eligibility on doctor visits in two ways:\n","\n","1) Via OLS regression where you use machine learning to control for the additional covariates.\n","\n","2) Via instrumental variables regression using the lottery indicator as an instrument for Medicaid eligibility where you use machine learning to control for the additional covariates in both the first stage and reduced form. Note that this is a different application of machine learning to instrumental variables than we learned in class. We are not using machine learning to solve the \"many instruments\" problem. Rather we are using machine learning to flexibly control for additional covariates in the instrumental variables estimation."]},{"cell_type":"markdown","source":["# Double Debiased with sample splitting (OLS regression method)"],"metadata":{"id":"7Kkfp1gT8No5"}},{"cell_type":"code","source":["#import the packages you need\n","from sklearn.model_selection import KFold\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import RidgeCV\n","from sklearn.linear_model import LinearRegression"],"metadata":{"id":"0-Xcs-7wBo8N","executionInfo":{"status":"ok","timestamp":1639518899957,"user_tz":420,"elapsed":8,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#I'll compare the performance of lasso and ridge in predicting y. I'll do this by splitting in test and training data and seeing which performs better. \n","#The model with better performance I will then use to do double debiased machine learning\n","#I'll do double debiased in order to control for many covariates with machine learning and use OLS to get the causal effect\n","# I'll first do it by hand and then with sample splitting."],"metadata":{"id":"22KeJFQ6o9el","executionInfo":{"status":"ok","timestamp":1639518899958,"user_tz":420,"elapsed":7,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#get train test split\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, D_train, D_test, y_train, y_test = train_test_split(x_scaled,D,y,random_state=42)"],"metadata":{"id":"SPmYhyLopoR0","executionInfo":{"status":"ok","timestamp":1639518899958,"user_tz":420,"elapsed":7,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#start with lasso cross validation\n","from sklearn.linear_model import LassoCV\n","from sklearn.metrics import mean_squared_error\n","\n","lassoxy = LassoCV(cv=5, random_state=0).fit(X_train, y_train)\n","\n","print(lassoxy.score(X_test, y_test))\n","\n","lasso_predict = lassoxy.predict(X_test)\n","lasso_mse = mean_squared_error(y_test, lasso_predict)\n","print(\"lasso mse\",lasso_mse)\n","lassoxy.coef_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42uMPuhHpwIK","executionInfo":{"status":"ok","timestamp":1639518900204,"user_tz":420,"elapsed":253,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"495b9e20-28b8-4c57-d94e-860a4ada5c70"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["0.025083292065102847\n","lasso mse 8.000721197957448\n"]},{"output_type":"execute_result","data":{"text/plain":["array([ 0.06647315, -0.06206118,  0.1660925 , -0.10408818, -0.25206037,\n","        0.        ,  0.15258452, -0.        ,  0.04064783, -0.        ,\n","       -0.        ,  0.        ])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#Do CV for best parameters of ridge of x on y\n","bestRidgey = RidgeCV(alphas=[.01, .1, .3,.5,.8,1]).fit(X_train, y_train)\n","\n","print(bestRidgey.score(X_test, y_test))\n","\n","ridge_predict = bestRidgey.predict(X_test)\n","ridge_mse = mean_squared_error(y_test, ridge_predict)\n","print(\"ridge mse\",ridge_mse)\n","\n","print(\"Best Alpha: \", bestRidgey.alpha_)\n","print(bestRidgey.coef_)\n","\n","#Ridge had a slightly better score and mse after cross validation so I will use that model going forward"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVNi7V0QqeRb","executionInfo":{"status":"ok","timestamp":1639518900205,"user_tz":420,"elapsed":16,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"81b2840d-e7ba-4c80-9d75-6a9065470b22"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0.025855805655442765\n","ridge mse 7.99438151190259\n","Best Alpha:  1.0\n","[ 0.09030249 -0.0786514   0.18514746 -0.12438025 -0.26906333  0.01795229\n","  0.19691888 -0.00268324  0.06862871  0.00897948 -0.01047143  0.02656548]\n"]}]},{"cell_type":"code","source":["#fit the optimal ridge regression on the whole data set (x on y) and show how well it does\n","#this is regression of x on y is the first step of DDML\n","bestRidgey.fit(x_scaled,y)\n","\n","ridge_predict = bestRidgey.predict(x_scaled)\n","ridge_mse = mean_squared_error(y, ridge_predict)\n","print(\"ridge mse\",ridge_mse)\n","\n","print(\"Best Alpha: \", bestRidgey.alpha_)\n","print('coefficients',bestRidgey.coef_)\n","print('score',bestRidgey.score(x_scaled, y))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUgFKqru9O3w","executionInfo":{"status":"ok","timestamp":1639518900205,"user_tz":420,"elapsed":12,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"6279b19f-7bd4-40a0-d187-35b9e4dc268d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["ridge mse 6.853705456031021\n","Best Alpha:  1.0\n","coefficients [ 0.13735499 -0.1003574   0.19094071 -0.1197136  -0.26133075  0.01402469\n","  0.18067523 -0.00554334  0.03792032 -0.00613201  0.00860593  0.02579008]\n","score 0.029235067825198247\n"]}]},{"cell_type":"code","source":["#get residuals from the regression\n","yresid=y-bestRidgey.predict(x_scaled)"],"metadata":{"id":"j9BHTaSk-0bC","executionInfo":{"status":"ok","timestamp":1639518900206,"user_tz":420,"elapsed":10,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#Do CV for best parameters of ridge of x on d\n","bestRidged = RidgeCV(alphas=[.01, .1, .3,.5,.8,1]).fit(X_train, D_train)\n","\n","print(bestRidged.score(X_test, D_test))\n","\n","ridge_predict = bestRidgey.predict(X_test)\n","ridge_mse = mean_squared_error(D_test, ridge_predict)\n","print(\"ridge mse\",ridge_mse)\n","\n","print(\"Best Alpha: \", bestRidged.alpha_)\n","print(bestRidged.coef_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Yqr3uNKwjUk","executionInfo":{"status":"ok","timestamp":1639518900206,"user_tz":420,"elapsed":9,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"9fa72e28-176a-40af-a4d1-747f41cdde34"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["0.16094459461786537\n","ridge mse 2.6119442666522885\n","Best Alpha:  1.0\n","[ 0.14305    -0.02057878  0.01382168  0.03507404 -0.07554521 -0.00899107\n"," -0.00523283  0.00057069  0.00038693  0.00017928 -0.00461706  0.00485552]\n"]}]},{"cell_type":"code","source":["#fit the optimal ridge regression on the whole data set (x on d) and show how well it does\n","#this is regression of x on d is the second step of DDML\n","bestRidged.fit(x_scaled,D)\n","\n","ridge_predict = bestRidgey.predict(x_scaled)\n","ridge_mse = mean_squared_error(D, ridge_predict)\n","print(\"ridge mse\",ridge_mse)\n","\n","print(\"Best Alpha: \", bestRidged.alpha_)\n","print('coefficients',bestRidged.coef_)\n","print('score',bestRidged.score(x_scaled, D))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t81Vd9iWw3mB","executionInfo":{"status":"ok","timestamp":1639518900398,"user_tz":420,"elapsed":199,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"b81550ad-d5c0-40da-cec7-a4131b6d25ec"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["ridge mse 2.6299602628607865\n","Best Alpha:  1.0\n","coefficients [ 0.14291013 -0.02170197  0.01407619  0.03555553 -0.076947   -0.00578546\n","  0.00717022  0.00587518  0.00578178  0.00204862 -0.00622726  0.01108803]\n","score 0.15429044285108928\n"]}]},{"cell_type":"code","source":["# get residuals of D from d_hat\n","\n","dresid = D - bestRidged.predict(x_scaled)\n","\n","#OLS regress y residuals on d residuals\n","ddmlregtest =LinearRegression().fit(dresid.to_numpy().reshape(-1,1),yresid)\n","ddmlregtest.coef_[0]\n","\n","#now that I've created an estimate the basic way, I'll try sample splitting"],"metadata":{"id":"5bM3Fi_q-9qF","executionInfo":{"status":"ok","timestamp":1639518900591,"user_tz":420,"elapsed":200,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4442ee63-0b80-415f-c7ea-42c6e1e72fdc"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0690422147587486"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["x_scaled = pd.DataFrame(x_scaled, columns = list(X.columns))"],"metadata":{"id":"c9RIZN0y_gTB","executionInfo":{"status":"ok","timestamp":1639518900593,"user_tz":420,"elapsed":5,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\n","\n","#try with sample splitting\n","\n","# create our sample splitting \"object\"\n","kf = KFold(n_splits=5,shuffle=True,random_state=42)\n","\n","# apply the splits to our Xs\n","kf.get_n_splits(x_scaled)\n","\n","# create an array to hold each fold's regression coefficient\n","coeffs=np.zeros(5)\n","\n","# loop through each fold\n","ii=0\n","for train_index, test_index in kf.split(x_scaled):\n","  X_train, X_test = x_scaled.iloc[train_index,:], x_scaled.iloc[test_index,:]\n","  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","  d_train, d_test = D.iloc[train_index], D.iloc[test_index]\n","\n","  #double debiased machine learning process:\n","\n","  # Ridge y on training folds:\n","  bestRidgey.fit(X_train, y_train)\n","\n","  # but get residuals in test set\n","  yresid=y_test-bestRidgey.predict(X_test)\n","  \n","  #Ridge d on training folds\n","  bestRidged.fit(X_train, d_train)\n","\n","  #but get residuals in test set\n","  dresid=d_test-bestRidged.predict(X_test)\n","\n","  # regress resids on resids\n","  ddml=LinearRegression().fit(dresid.to_numpy().reshape(-1,1),yresid)\n","\n","  # save coefficient in a vector\n","  coeffs[ii]=ddml.coef_[0]\n","  ii+=1\n","\n","# Take average\n","print(\"Double-Debiased Machine Learning effect of medicaid: {:.3f}\".format(np.mean(coeffs)))\n","coeffs\n","\n","#the results with and without sample splitting are very similar, both very close to 1.06"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_aHSJAa8NAx","executionInfo":{"status":"ok","timestamp":1639518900840,"user_tz":420,"elapsed":252,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"1c6af55c-89c1-4bbe-e47f-be9afa89924f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Double-Debiased Machine Learning effect of medicaid: 1.056\n"]},{"output_type":"execute_result","data":{"text/plain":["array([1.10510928, 0.87950401, 1.01571054, 1.32930309, 0.94879214])"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["# Instrumental Variables Method"],"metadata":{"id":"BYXVu-hG4r8w"}},{"cell_type":"markdown","source":["This IV implementation is different than we have done in class. The instructions say to include the X covariates in both the first and second stage. Under the assumption of sparsity and to combat overfitting, I'll use lasso to decide which variables to keep in each stage. Then I'll do a simple OLS which will let me compare the coefficients to get the ratio/wald estimator for the instrument."],"metadata":{"id":"fSlq0mkC8oW2"}},{"cell_type":"code","source":["#First Stage predict D with Z to get D-hat\n","from sklearn.linear_model import Lasso\n","#create matrix with z and x\n","\n","\n","print(Z.shape)\n","print(X.shape)\n","X['wonlottery'] = Z\n","X.shape\n","\n","fs_x = X\n","print(fs_x.columns)\n","lassofs = Lasso(alpha=.02).fit(fs_x,D) #lasso to see which vars to keep\n","print(lassofs.coef_)\n","\n","retained_fs = fs_x.iloc[:,(lassofs.coef_!=0)]\n","retained_fs.columns #This leaves only three columns which we can use in an OLS first stage"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHyO1VyE2sa2","executionInfo":{"status":"ok","timestamp":1639518918995,"user_tz":420,"elapsed":155,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"adc30041-2d63-486a-b560-13fc4b08fcf8"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["(4666,)\n","(4666, 12)\n","Index(['wonlottery', 'numhhmembers', 'female_6m', 'birthyear_6m',\n","       'employ_hrs_6m', 'race_hisp_6m', 'race_white_6m', 'race_black_6m',\n","       'race_amerindian_6m', 'race_asian_6m', 'race_pacific_6m',\n","       'race_other_qn_6m'],\n","      dtype='object')\n","[ 0.20868556 -0.          0.          0.00269882 -0.04942463 -0.\n","  0.          0.          0.         -0.         -0.          0.        ]\n"]},{"output_type":"execute_result","data":{"text/plain":["Index(['wonlottery', 'birthyear_6m', 'employ_hrs_6m'], dtype='object')"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["#now do OLS first stage with retained x's and z on d\n","#get retained x's\n","ols_fs = LinearRegression().fit(retained_fs,D) #using ols allows me to still claim an effect that isn't biased under the right assumptions\n","z_coef = ols_fs.coef_[0]\n","#get d_hat\n","d_hat = ols_fs.predict(retained_fs)"],"metadata":{"id":"Q57XruWWtWZT","executionInfo":{"status":"ok","timestamp":1639518929458,"user_tz":420,"elapsed":164,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#second stage, predict y with D-hat and X's\n","\n","#first get the matrix of x's and d-hat\n","\n","X = X.drop(['wonlottery'], axis=1)\n","X['d_hat'] = d_hat\n","\n","ss_x = X\n","print(ss_x.columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTHZhFuNzy0Q","executionInfo":{"status":"ok","timestamp":1639518934036,"user_tz":420,"elapsed":246,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"5d7af574-e9b9-4f97-a2a7-c6df2880b310"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['numhhmembers', 'female_6m', 'birthyear_6m', 'employ_hrs_6m',\n","       'race_hisp_6m', 'race_white_6m', 'race_black_6m', 'race_amerindian_6m',\n","       'race_asian_6m', 'race_pacific_6m', 'race_other_qn_6m', 'd_hat'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["#then do lasso to see which variables should be retained\n","\n","lasso_ss = Lasso(alpha=.02).fit(ss_x,y)\n","\n","print(lasso_ss.coef_)\n","\n","retained_ss = ss_x.iloc[:,(lasso_ss.coef_!=0)]\n","retained_ss.columns #This leaves only three columns which we can use in an OLS second stage\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYbCmcvstvwO","executionInfo":{"status":"ok","timestamp":1639518937568,"user_tz":420,"elapsed":172,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"4ab704d0-0dae-49e9-872d-43bf4fc76c47"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.14463014  0.31425175 -0.00986284 -0.19370512 -0.          0.30020979\n"," -0.          0.         -0.          0.         -0.          0.0280126 ]\n"]},{"output_type":"execute_result","data":{"text/plain":["Index(['numhhmembers', 'female_6m', 'birthyear_6m', 'employ_hrs_6m',\n","       'race_white_6m', 'd_hat'],\n","      dtype='object')"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["#do ols second stage with retained variables\n","ols_ss = LinearRegression().fit(retained_ss,y)\n","d_hat_coef = ols_ss.coef_[5]\n","\n","#divide the two coefficients to get the effect\n","effect = d_hat_coef/z_coef\n","print('effect:',effect)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1WNFG1duFDe","executionInfo":{"status":"ok","timestamp":1639518941143,"user_tz":420,"elapsed":161,"user":{"displayName":"Abe Burton","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01633280074345084238"}},"outputId":"a3b3e1a6-44b3-4af5-e18a-d07fcd7d61df"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["effect: 3.231792596213835\n"]}]},{"cell_type":"markdown","metadata":{"id":"6KuE2vaK0PUC"},"source":["## Hints and Requirements"]},{"cell_type":"markdown","metadata":{"id":"0r9xBEww0Sty"},"source":["*   Thoroughly document your code with comments explaining what each part of your code is doing\n","\n","*   Be sure to \"print\" all of the relevant results after estimating/calculating them\n","\n","*   Use best practices that we have learned this semester, including pre-processing variables as necessary and choosing tuning parameters.\n","\n","*   Hint: the dataset contains missing values for many of the variables and many of the observations. You may assume that observations are missing at random, and therefore restricting your dataset to observations for which no variables are missing is appropriate.\n","\n","*   Hint: the instrumental variables setting here is somewhat different from the \"many instruments\" situation we discussed during the semester. Here, you calculate the instrumental variables estimate by estimating the \"effect\" of the instrument on the outcome (the reduced form), and the \"effect\" of the instrument on the treatment variable (the first stage), and your instrumental variables estimate is the ratio of the reduced form to the first stage. You will use machine learning to control for the additional covariates in both the reduced form and first stage.\n","\n","*   Choose the machine learning method(s) you use based on what yields the best out-of-sample accuracy among at least two different machine learning methods, where out-of-sample accuracy is assessed using a held out test set."]}]}
